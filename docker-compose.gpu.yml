services:
  # 作业3：基于 LangGraph 的多角色协作智能体系统（GPU版本）
  homework3-langgraph-agents-gpu:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: homework3-langgraph-agents-gpu
    volumes:
      # 挂载数据目录（数据库文件）
      - ./data:/app/data
      # 挂载日志目录
      - ./logs:/app/logs
      # 挂载缓存目录
      - ./cache:/app/cache
      # 注意：.env 文件不挂载，环境变量通过 environment 传递
    shm_size: '2gb'  # 共享内存大小
    environment:
      - PYTHONPATH=/app
      - PYTHONUNBUFFERED=1
      - PYTHONIOENCODING=utf-8
      - CUDA_VISIBLE_DEVICES=0
      # LLM API 配置（从环境变量读取）
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY:-}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - LLM_PROVIDER=${LLM_PROVIDER:-deepseek}
      - LLM_MODEL=${LLM_MODEL:-deepseek-chat}
      - LLM_TEMPERATURE=${LLM_TEMPERATURE:-0.7}
      - LLM_MAX_TOKENS=${LLM_MAX_TOKENS:-2000}
      # 数据库配置
      - DATABASE_URL=sqlite:///./data/conversations.db
      # MCP 工具配置
      - MCP_SERVER_URL=${MCP_SERVER_URL:-http://localhost:8001}
      - KNOWLEDGE_BASE_URL=${KNOWLEDGE_BASE_URL:-http://localhost:8002}
      - ORDER_SERVICE_URL=${ORDER_SERVICE_URL:-http://localhost:8003}
      # 日志配置
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - LOG_FILE=/app/logs/app.log
    ports:
      # FastAPI 服务端口
      - "8000:8000"
    restart: unless-stopped
    # NVIDIA GPU支持（RTX 5080）
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "python", "-c", "import torch; print('GPU available:', torch.cuda.is_available())"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    stdin_open: true
    tty: true
    networks:
      - homework3-network

networks:
  homework3-network:
    driver: bridge
